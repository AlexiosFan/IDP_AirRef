\documentclass[12pt,letterpaper]{article}

\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{cleveref}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{float}

\newcommand*{\bigo}{\mathcal{O}}
\newcommand*{\R}{\mathbb{R}}
\newcommand{\Poly}{\mathcal{P}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}[theorem]{Observation}

\setlength\parindent{0pt}

\title{Computing the convex hull of solution vectors to the airplane refueling problem to find a general description}
\author{Zixuan Fan}

\begin{document}
\maketitle

\section{Motivation}

\subsection{Problem Statement}
The Airplane Refueling Problem is a combinatorial problem that is 
neither known to be NP-complete nor polynomially solvable. 
It reflects a real-life problem, where a fleet of airplanes refuels each other 
to maximize the flying distance. The earliest mention of this problem stems 
from \textit{Puzzle-Math} \cite{gamow1958puzzle} by George Gamow and Marvin Stern published in 1958.    
Woeginger introduced the a formalization of this problem, 
we present a definition based on his work. \cite{woeginger2010scheduling}
\begin{definition}
 Given $n$ planes each with a volume $w_j$ and consumption rate $p_j$,
 what is the longest time/distance that this fleet of planes can fly with full volumes,
 if they can refuel each other?
\end{definition}
An intuitive idea is consuming the volume of only one plane 
at a time, and dropping it out after its volume is empty. This approach 
finds a \textit{drop out ordering} $\sigma$
that maximizes the objective function 
\begin{align*}
    \max \sum_{j=1}^{n} \dfrac{w_{\sigma(j)}}{\sum_{k = j}^n p_{\sigma(k)}}
\end{align*}
To elaborate on this objective function, only the planes with indexing no less than $j$ in order $\sigma$
are consuming the volume of the $j$-th plane. Thus, the time until the $j$-th fuel is fully consumed is 
\begin{align*}
    \dfrac{w_{\sigma(j)}}{\sum_{k = j}^n p_{\sigma(k)}}
\end{align*}
while the sum of all these times is the objective maximization function.

\subsection{Scheduling and Mathematical Programming}
Based on understaning mentioned above, Wiebke HÃ¶hn introduced a formalisation of this problem as a scheduling problem. \cite{hohn2015performance}
We would briefly introduce the scheduling problem, and explain how the problem fits in this context. 
\begin{definition}
    Given a cost function $f$ and a sequence of $n$ jobs, each with processing time $p_j$ and weight $w_j$, how to find a permutation $\pi$
    such that the objective function $\sum_{j = 1}^{n} w_j f(\pi, j)$ is minimized or maximized? 
\end{definition}
For simplicity, we also introduce the completion time $C_j$, i.e. the time until the $j$-th plane is consumed given permutation $\pi$.
\begin{align*}
    C_j = \sum_{k = 1}^j p_{\pi(k)}
\end{align*}
In our problem, we can view $\sigma$ as permutation $\pi$, the reverse of ordering.
\begin{align*}
    \sum_{j=1}^{n} \dfrac{w_{\sigma(j)}}{\sum_{k = j}^n p_{\sigma(k)}} = \sum_{j = 1}^n \dfrac{w_{\pi(j)}}{\sum_{k = 1}^j p_{\pi(k)}}
\end{align*}
Replacing the sum of processing time with completion time, we obtain a cost function $f(\pi, j) := C_{\pi(j)}$
and the objective function in a simpler form.
\begin{align*}
    \sum_{j = 1}^n \dfrac{w_{\pi(j)}}{\sum_{k = 1}^j p_{\pi(k)}} = \sum_{j = 1}^n \dfrac{\pi(j)}{C_{\pi(j)}} =  \sum_{j = 1}^n \dfrac{w_{j}}{C_j}
\end{align*}
With all premises explained, we can finally introduce a mathematical programming formulation. Note that $S_n$ stands for the permutation group of order $n$. 
\begin{align*}
    &\max w^T \cdot x \label{eq:linobj} \tag{B} \\ 
    \text{s. t.\ }& C_{\pi(j+1)} - C_{\pi(j)} = p_{\pi(j)} \forall j \\
    & \pi \text{ is a permutation} \\ 
    & x_j = \dfrac{1}{C_j} \ \ \forall j \in [n] 
\end{align*}
Should this be a linear program, we can solve the problem in polynomial time. 
However, the constraints are not linear, for they are based on the permutations.
Hence, our primitive goal is finding an equivalent linear program to this formulation. In addition, 
we are also interested in the potential of convex programming in this problem. 

\section{Theoretical Backgrounds}
Following the formulation, an intuition is computing the convex hull of feasible solutions, 
which are all possible combinations of the completion time's reciprocals. 
\begin{align*}
 s = \left( \dfrac{1}{C_1}, \dfrac{1}{C_2}, ..., \dfrac{1}{C_n}\right)
\end{align*} 
We can observe that each solution vector corresponds to a permutation of the consumption rate. 
More precisely, the longer the completion time, the later the volume is consumed in the permutation.
There are $n!$ many of these vectors, and the convex hull is a polytope in $\R^n$. 
Thus, we want to explore the special properties of this polytope.

\subsection{Description of the Polytope}
We refer to a convex polytope $\Poly$ as a convex set in $R^n$ Euclidean space describing a polytope. 
There are two ways to describe a polytope.
\begin{enumerate}
    \item $V$-Representation: A convex hull of finite set of points. 
    \begin{align*}
        \Poly = \text{conv}\{v_1, \cdots, v_n \}
    \end{align*}
    \item $H$-Representation: A finite sets of inequalities, i.e. the intersection of half-spaces
    \begin{align*}
        \Poly = \{x \in \R^n | Ax \leq b \}
    \end{align*}
\end{enumerate}
It is known that the two representations are equivalent. 
\begin{theorem}
    \label{thm:polytope}
    For each polytope $\Poly$ in $\R^n$, there is an $H$-polytope $\Poly$'s 
    describing exactly the same polytope as $\Poly$ and vice versa.
\end{theorem}
In practice, theorem \ref{thm:polytope} is the theoretical basis for us to 
compute the convex hull of solution vectors as a set of inequalities.


\subsection{Permutation Matrices}
Before we take a closer look at permutahedra, we need to introduce permutation matrices.
\begin{definition}
    A permutation matrix $M \in \R^{n \times n}$ is a square binary matrix describing a permutation $\pi$.
    For each entry $m_{ij}$ of $M$, the following constraints must hold.
    \begin{enumerate}
        \item $m_{ij} \in \{0, 1\}$ for all $i, j$
        \item $\sum_{i = 1}^n m_{ij} = 1$, for all $j$
        \item $\sum_{j = 1}^n m_{ij} = 1$, for all $i$
    \end{enumerate}
\end{definition}
First, we show that there is a bijective relationship between permutation matrices and permutations, i.e. permutation matrices 
can be used to describe permutations.
\begin{lemma}
    There is a bijective mapping between permutation matrices and permutations.
\end{lemma}
\begin{proof}
    For each permutation $\pi \in S_n$, we construct a permutation matrix $M$.
    \begin{align*}
        m_{ij} = 
        \begin{cases}
            1 & \text{if } \pi(j) = i \\
            0 & \text{otherwise}
        \end{cases}
    \end{align*}
\end{proof}
A property of permutation matrices is that they are doubly stochastic, i.e., the sum of each row and each column is 1.
In the special case of permutation matrices, the matrix is also orthogonal.
\begin{lemma}
    If $M$ is a permutation matrix, then $M$ is orthogonal.
\end{lemma}
\begin{proof}
    We consider the matrix product $MM^T$. It suffices to show $MM^T = I$. The $(i, j)$-th entry of $MM^T$ is given by 
    \begin{align*}
        (MM^T)_{ij} = \sum_{k = 1}^n m_{ik}m_{jk}
    \end{align*}
    If $i \neq j$, then we are summing up the product of entries in two different rows. 
    Since there is only one entry equal to one in each column, the products are all zero. Hence, their sum is also zero. \\ 
    If $i \neq j$, then we are summing up the product of entries in the same row.
    There is only one entry equal to one in each row, so the product is one. \\
    In conclusion, the matrix product is the identity matrix, and $M$ is orthogonal.
\end{proof}
\begin{corollary}
    \label{cor:inverse}
    If $M$ is a permutation matrix, then $M^{-1} = M^T$.
\end{corollary}


\subsection{Permutahedron}
A permutahedron is a polytope that is the convex hull of all vertices representing a permutation i.e. $\{(\pi(1), \pi(2), \cdots, \pi(k) | \pi \in S_n) \}$. \cite{doi:10.1137/0122054}
In the permutahedron of the $n$-th order $P_n$, each vertex is a permutation of $n$ elements.
Hence, there are $n!$ many vertices in $P_n$. For example, a hexagon is a second-order permutahedron. 
First, we take a look at a simple LP formulation of permutahedra. 
\begin{align*}
    \sum_{i \in S} x_i = \sum_{k = n - |S| + 1}^n k, \ \ \forall \emptyset \neq S \subset [n]
\end{align*}
We need $2^n - 2$ facets to define the polytope.
\\
Although the number of faces is not polynomial in $n$,
some symmetric properties were discovered and are useful for optimization. Some researchers\textbf{cite later}
have found an extended formulation $\R^{k^2+k} \rightarrow \R^{k}$, which allows us 
to solve the LP on $P_k$ in polynomial time. 

\begin{align}
    &\sum_{j \in [n]} y_{ij} = 1, \ \ \forall i \in [n], \sum_{i \in [n]} y_{ij} = 1, \ \ \forall j \in[n] \label{eq:C} \tag{C} \\ 
    &y_{ij} \geq 0, \ \ \forall i, j \in [n] \nonumber 
\end{align}
The trick is the permutation matrix. We show that this is a formulation of the 
convex hull of all permutation metrices.
\begin{lemma}
    The polytope defined by \ref{eq:C} is the convex hull of all permutation matrices to order $n$.
\end{lemma}
\begin{proof}
    It suffices to show that \ref{eq:C} describes the set of all convex combinations of permutation matrices. \\
    $\impliedby$: Let $M^1, M^2, \cdots M^k$ be arbitrary distinct permutation matrices. Let $\lambda_1, \lambda_2, \cdots \lambda_k \in [0, 1]$ be arbitrary
    such that $\sum_{i=1}^k = 1$.
    We obtain a convex combination $Y = \sum_{l = 0}^k \lambda_l M^i$.We check each constraints accordingly. 
    \begin{align*}
    \sum_{j \in [n]} y_{ij} &= \sum_{j \in [n]} \sum_{l = 0}^k \lambda_l m_{ij}^l \\
    &= \sum_{l = 0}^k \lambda_l \sum_{j \in [n]} m_{ij} = \sum_{l = 0}^k \lambda_l = 1 \\ 
    \sum_{i \in [n]} y_{ij} &= \sum_{i \in [n]} \sum_{l = 0}^k \lambda_l m_{ij}^l \\
    &= \sum_{l = 0}^k \lambda_l \sum_{i \in [n]} m_{ij} = \sum_{l = 0}^k \lambda_l = 1 \\ 
    y_{ij} &= \sum_{l = 0}^k \lambda_k m_{ij}^l \geq \sum_{l = 0}^k \lambda_k \cdot 0  = 0 
    \end{align*}
    We may conclude that $Y$ is in the polytope described by \ref{eq:C}. \\
    $\implies$: Let $Y$ be a feasible solution to constraints in \ref{eq:C}. 
    We observe that $Y$ is doubly stochastic. It is shown by Birkhoff's theorem \textbf{cite Birkhoff} that 
    $Y$ is a convex combination of permutation matrices. 
\end{proof}

As a consequence, we can obtain the vertices representing the permutations by 
\begin{align*}
    x_i = \sum_{j = 0}^n j \cdot y_{ij} \forall i \in[n]
\end{align*}
In this way, we are able to reduce the number of constraints to $n^2 + 3n$ under $R^{n^2 + n}$.
This extended formulation is helpful in terms of both linear programming and non-linear programming. 

\section{Attempts with non-linear programming}
Inspired by the permutahedron, we want to find a similar formulation for the polytope of the completion time,
because there is a permutation-related structure in the polytope. 

\subsection{Formulation}
Recall in \ref{eq:linobj}, the completion time $C_j$ is 
an acculumating sum of the processing time $p_j$ with regard to 
permutation $\pi$. We would exploit the formulation in \ref{eq:C} 
to find a similar formulation for the completion time. The figure \ref{fig:completion}
shows how the completion time is constructed with a permutation, and respectively with a permutation matrix.
\begin{figure}[h!]
    \centering
    \begin{tikzcd}
        \begin{pmatrix}
            p_1 \\ 
            \vdots \\
            p_n
        \end{pmatrix} 
        \arrow[r, "\pi"]
        & \begin{pmatrix}
            p_{\pi(1)} \\ 
            \vdots \\
            p_{\pi(n)}
        \end{pmatrix}
        \arrow[r, "sum"]
        & \begin{pmatrix}
            p_{\pi(1)} \\ 
            \vdots \\
            \sum_{i = 0}^n p_{\pi(i)}
        \end{pmatrix} = 
        \begin{pmatrix}
            C_{\pi(1)} \\
            \vdots  \\
            C_{\pi(n)}
        \end{pmatrix}
        \arrow[r, "\pi^{-1}"]
        & \begin{pmatrix}
            C_1 \\ 
            \vdots \\
            C_n
        \end{pmatrix}
    \end{tikzcd}
    \caption{Construction of completion time}
    \label{fig:completion}
\end{figure}
Apparently, such operations can be formulated with matrices. 
We introduce a matrix $S$ for the summation operation. $S$ is a lower-triangular matrix with all ones.
In addition, we retain the permutation matrix as shown in \ref{eq:C}.
For naming conventions of variables, we use $y_i$ for completion and $z_i$ for permutation matrices 
in the following formulation.
\begin{align*}
    \max w^Tx& \text{ s.t. } \label{eq:D} \tag{D} \\
 x_i y_i &= 1 \ \forall i, \ \ y \geq 0, \\ 
    \sum_{i \in [n]} z_{ij} &= 1 \forall j,  \ \ 
    \sum_{j \in [n]} z_{ij} = 1 \forall i,  \ z_{ij} \in \{0, 1\} \forall i, j \\ 
    \Pi &= (z_{ij}), S = \begin{pmatrix}
        1 & \cdots & 0 \\ 
        \cdots & \cdots & \cdots \\ 
        1 & \cdots & 1
    \end{pmatrix} \\
 y &= \Pi^T S \Pi p 
\end{align*}
One special part of the formulation is $\Pi^T$, where the transpose 
of the permutation matrix is used. This stems from the result in \ref{cor:inverse},
where the inverse of a permutation matrix is its transpose.

\subsection{Convexity}
Now that we have a correct formulation, we are ready to implement and execute some experiments.
However, the given formulation serves as a mixed-integer nonlinear program. 
For actual execution, we have first convert it into a non-integer program, and check its convexity afterwards.
In a classical manner, the binary integer variables are relaxed to continuous variables.
\begin{align*}
 z_{ij} \in [0, 1] \ \forall i, j
\end{align*}
There are also quadratic equality constraints, where we lose the convexity for the equal relation. 
There are roundabouts to make them convex, by introducing inequalities. 
\begin{align*}
 x_i y_i &\geq 1 \ \forall i \\
 y &\geq \Pi^T S \Pi p
\end{align*}
While we can easily agree that linear constraints are convex, we would like to 
show that these quandratic inequalities are convex.
\begin{lemma}
    The introduced quadratic inequalities are convex.
\end{lemma}
\begin{proof}
    We start with the first inequality. Let $i, j$ be arbitrary.
    By assumption, it holds that $x_iy_i \geq 1$, $x_jy_j \geq 1$ and $x, y \geq 0$.
    For any $\lambda \in [0, 1]$, it follows 
    \begin{align*}
        &(\lambda x_i + (1-\lambda) x_j)(\lambda y_i + (1-\lambda) y_j) \\
        &= \lambda^2 x_i y_i + (1-\lambda)^2 x_j y_j + \lambda(1-\lambda)(x_iy_j + x_jy_i) \\
        &\geq \lambda^2 + (1-\lambda)^2 + \lambda(1-\lambda)(x_iy_j + x_jy_i) \\ 
        &\geq \lambda^2 + (1-\lambda)^2 + \lambda(1-\lambda)(\dfrac{y_j}{y_i} + \dfrac{y_i}{y_j}) \\ 
        &\geq \lambda^2 + (1-\lambda)^2 + 2\lambda(1-\lambda) \\ 
        &= \lambda^2 + 1 - 2\lambda(1-\lambda) + \lambda^2 + 2\lambda(1-\lambda) \\ 
        &= 1 + 2\lambda^2 \geq 1
    \end{align*}
    Now we consider the second inequality. Recall that $\Pi^T = \Pi^{-1}$.
    We can rewrite the inequality as
    \begin{align*}
        \Pi y - S\Pi p \geq 0
    \end{align*}
    Note that the term $S\Pi p$ is linear for $z$'s and $y$'s.
    Thus, we only have deal with the term $\Pi y$. We take a closer look at this product.
    \begin{align*}
        \Pi y &= \begin{pmatrix}
            \sum_{j = 0}^{n} z_{1j}y_j \\
            \vdots \\
            \sum_{j = 0}^{n} z_{nj}y_j
        \end{pmatrix}
    \end{align*}
    Analogously to the first inequality, we can show that $z_{ij}y_j \geq 0$ is convex. 
    By adding up all these inequalities, we obtain that $\sum_{j = 0}^{n} z_{ij}y_j \geq 0$ is also convex.
    In conclusion, the inequality $\Pi y - S\Pi p \geq 0$ is convex.
\end{proof}
In this way, we can obtain a feasible, convex, but \textbf{unbounded} area. 
\begin{lemma}
    The feasible are of the given convex program is unbounded.
\end{lemma}
\begin{proof}
    Suppose for contradiction that the feasible area is bounded.
    Specifically, we would show that $y$ is unbounded. Hence, 
    there exists by assumption a constant $M$ such that $|y| \leq M$ for all 
    feasible $y$. \\ 
    Let $\hat{y}$ be arbitrarily feasible. We construct a new feasible $\hat{y}'$ by 
    \begin{align*}
        \hat{y}' = \hat{y} + M \cdot \vec{\mathbf{1}}
    \end{align*} 
    Now we check all three constraints concerning $\hat{y}'$.
    We start with non-negativity.
    \begin{align*}
        \hat{y}' &= \hat{y} + M \cdot \vec{\mathbf{1}} \geq \hat{y} \geq 0
    \end{align*}
    For $x_iy_i \geq 1$, increasing $\hat{y}_i$ by $M$ also results in a valid $\hat{y}_i$.
    Analogously, the second inequality is also satisfied. Thus, $\hat{y}'$ is feasible. \\
    It follows 
    \begin{align*}
        |\hat{y}'| = |\hat{y} + M \cdot \vec{\mathbf{1}}| \geq |\hat{y}| + M \cdot n > M
    \end{align*}
    which is a contradiction. Hence, the feasible area is unbounded.
\end{proof}

\textbf{Unboundedness} is another problem, for we objective function is maximized. 
Thus, any attempts on convex optimization would maximize the objective function to infinitely.
This discovery also hints that relaxing this non-convex problem 
to a convex version might always result in an unbounded feasible area.
Unfortunately, we cannot solve the problem with this non-linear programming approach. 

\subsection{Simple Practical Experiments}
Although the program is not convex, we can still run it in some scientific computing software.
For software based on Disciplined Convex Programming, like CVXPY \cite{diamond2016cvxpy}, we cannot even run the program 
because it has semantic checks on convexity. For software that does not require this check, like SCIP \cite{BolusaniEtal2024ZR}, the program 
ends up in a dead loop without termination. 

\section{Attempts with linear programming}
Since finding a convex formulation directly from permutahedra is impossible, 
we wonder if we can find the polytope describing the solution vectors in a linear programming way.
Without too many theoretical backgrounds, 
we decided to do some practical computation on those vertices.

\paragraph{Setups}
For the computation of the polytope with vertices, we use the scientific computing software polymake \cite{assarf2017computing}.
Polymake is a software that can compute the $H$-representation of a polytope given the $V$-representation. 
The computation was run on a private computer with Intel i5-12600k CPU with 16 GB RAM. 


\subsection{Input Generation}
As we have analyzed in the first attempt, we want to compute the convex hull of all solution vectors.
We name the polytope describing this convex hull as $\Poly$.
\begin{align*}
 s = \left( \dfrac{1}{C_1}, \dfrac{1}{C_2}, ..., \dfrac{1}{C_n} \right)
\end{align*}
Since the completion time is based on the permutation $\pi$, we need a few steps to generate all possible 
vectors. 
\begin{enumerate}
    \item Generate a randomized/special processing time $p_j$
    \item Pick a permutation $\pi$, arrange the consumption time in the order of $\pi$
    \item Compute the completion time by $C_1 = p_1$ and $C_{i+1} = C_i + p_{i+1}$
    \item Compute the inverse of $C_j$
    \item Repeat until all permutations are visited
\end{enumerate}
This results in $n!$ vertices in $\R^n$. A straightforward traversal on the vertices is not 
polynomial-time, but it may help us to find some polynomial-time structure within the polytope $\Poly$.

\subsection{First Computation}
\paragraph{Direct Observations}
Having tested a few randomly generated inputs, we have made some direct observations.
When several identical processing times are generated, the number of facets is largely reduced.
Suppose $n$ processing times were generated, while there were only $m$ unique values. 
The resulting polytope can be reduced to a polytope in $\R^m$.\\
The reason is also straightforward. If two identical values were generated, 
the number of unique solution vectors will also be reduced. Let $i$ and $j$ be 
arbitrary indices such that $p_i = p_j$. For each value of $C_i$ in permutation $\pi$, 
there is a $C'_j$ of the same value in another permutation $\pi'$. 
$C'_j$ and $\pi'$ can be generated by simply swapping $i$ and $j$ in the permutation $\pi$. \\
In conclusion, it makes sense that our computational experiments should be based 
on the number of unique values in the processing time. 
Thus, we only generated unique processing times in the following experiments. 

\paragraph{Running Time in Practice}
Although the computation of the polytope is polynomial-time in the number of vertices, our computation takes 
a significant amount of time. The reason is that the number of vertices is exponential in $n$. 
For the first few dimensions, the computation was quick and the results were as expected.
When we come to the fifth dimension, the computation takes a few seconds, while the sixth dimension 
takes around 1 minute to terminate. The computation of a 7-dimensional polytope does not terminate within 24 hours. 
This largely stops us from further computation in even higher dimensions. 
Luckily, we have obtained some raw data of the first 6 dimensions. 
The number of facets do not resemble a polynomial growth, hence we would expect an exponential growth in this case.

 
\subsection{Mass Computation for patterns}
The exponential growth of facets does not mean an expential decision prodecure for the polytope. 
Our second trial serves as mass computation in lower dimensions, where we want to obtain 
as many data as possible. In total, we have generated 10,000 inputs for 4-dimensional cases, 
and 1,000 inputs for 5-dimensional cases. \\
Based on those data, we would like to find a pattern in 
the number of facets. Thus, we establish a large amount of 
computation in $n = 4$ and $n = 5$. Some direct oberservations are shown in the following table. 
\begin{table}[ht]
    \centering
    \begin{tabular}{||c | c | c ||}
        \hline 
        $n$ & Number of Facets & Number of Unique Values\\
        \hline 
        \hline 
 2 & 2 & 1\\
        \hline 
 3 & 8 & 1\\
        \hline 
 4 & 67, 68, 69, 70 & 4\\
        \hline 
 5 & $700 \sim 800$ & $\geq 66$\\
        \hline 
 6 & $10000 \sim 12000$  & Unknown\\
        \hline 
    \end{tabular}
    \caption{Number of facets in the polytope for $n \leq 6$}
\end{table}
Starting from $n = 4$, the number of facets is no longer unique. 
Hence, we guess that the number of facets is somehow related to the input chosen. 
Unfortunately, a direct pattern was not observed for $n=5$.
One interesting case for $n = 4$ is when the number of facets is 67.
Among 10,000 inputs, only one input results in 67 facets. 

\paragraph{A Special Input}
We would like elaborate on this case to explain what we have observed.
The generated input for this 4d case is
\begin{align*}
    p_0 = (26, 35, 55, 81)
\end{align*}
By brute-force, we are able to obtain all possible values for each in dimension in 
our polytope. One numerical observation is $26+53 = 79$. If we recall 
the direction observation in the previous part, we can see this might result in 
a reduction in the number of solution vectors. In our results, the number of unique vectors is not reduced.
However, such relations reduces the number of higher dimensional faces. 
As a result, we have fewer facets.\\
Then we also tried to reproduce this result, it was discovered that the vector $p$ 
in satisfying the following property will result in the exactly 67 facets.
\begin{align*}
    p = (a, b, 3b - 2a, 3b - a)
\end{align*}
One simplest example is $p = (1, 2, 4, 5)$. 
We still fail to derive more general results from this observation.

\subsection{Patterns by Machine Learning}
Since no direct pattern was observed, we would like to use machine learning techniques to find a pattern in the data.
Both supervised and unsupervised learning techniques were exploited to find the pattern in the data. 
We focus on the case of $n = 4$, where the unique number of facets is few, and the data are easy to visualize.
We randomly generate each input of $v \in \R^4$ and call the Polymake interface for the polytope. In the end, 
we also process that data such that each dimension is a ratio between its original value and the first dimension. 
\begin{align*}
 (v_1, v_2, v_3, v_4) \rightarrow \left(1, \frac{v_2}{v_1}, \frac{v_3}{v_1}, \frac{v_4}{v_1} \right)
\end{align*}
Here are some results
\begin{enumerate}
    \item Linear Regression: The regression models do not fit well; we only obtain an accuracy barely above $0.5$.
    \item Logistic Regression: slightly better than linear regression, but still not good enough.
    \item Discriminant Analysis: We do not infer the column dependency to linear, so quandratic discriminant analysis was experimented.
    However, the approach is not really suitable for multimonial classification.
    \item Trees models: Tree-based models work best, but the tree's visualization is unclear, and we don't see much meaning behind it.
\end{enumerate}
In conclusion, the tree models tend to work better on the data, but due to the lack of features,
we cannot draw direct conclusion on the how the patterns can be discovered inside the data.
The fundamental machine learning techniques did not help us to discover the pattern, one 
would also consider deep learning techniques for this problem.

\subsection{Visualization and Discoveries}
Aside from machine learning, we also visualized the data to see if any pattern was observed, as shown in the figure below. 
A clear separation can be witnessed, but we cannot find a smooth curve separating the data, for outliers reside 
in the middle of the other category. 
\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.6]{3d_plot.png}
\end{figure*}
On the other hand, the data themselves also yield interesting properties. If we consider a solution vector 
\begin{align*}
 s = \left( \dfrac{1}{C_1}, \dfrac{1}{C_2}, ..., \dfrac{1}{C_n} \right)
\end{align*}
For every two dimensions $ i \neq j$, the volume of the $i-$th plane is consumed before the $j-$th plane, or vice versa.
Formally speaking, either $C_1 < C_2$ or vice versa, and there is an equal amount of them, separated by the hyperplane 
$x_i - x_j = 0$. This separation is a valuable property and an interesting technique called \textbf{hyperplane arrangements} 
endeavors to research it. We also tried a greedy algorithm on based on this discovery, but the results 
were not satisfactory. The greedy search does not yield the exact solution already in the 6-dimensional case.
\textbf{EXPAND FOR DETAILS IN GREEDY SEARCH?}

\subsection{Formulation with Vasquez's Idea}
As an introduction, Vasquez's research \cite{vasquez2015airplane} on this problem was carefully studied 
in the early phase of this project. Later on, we re-dived into the paper for inspirations.
We also attempted to find a formulation based on his idea, but there was no success or flawed formulation. 
Hence, we do not bother to go into details. 

\section{Summary}
In this project, we reviewed a few pieces of literature on the airplane refueling problem and 
attempted to find a mathematical programming formulation for the problem. 
We gave a valid formulation, but it was not convex, and we were not able to solve it in polynomial time. 
This intractability is also evidence showing that this direction may not work.
\begin{enumerate}
    \item On the one hand, if we try to formulate this problem as a non-linear program problem, we end up 
 with a maximization problem over an unbounded convex feasible area or a bounded non-convex area. 
    \item On the other hand, if we try to find a linear programming formulation, in other words, a describing polytope, 
 it takes us exponential time to compute all facets in exchange for facets. 
\end{enumerate}
We can observe the first point from the first attempt on permutahedra, and we want to elaborate on the second point. 
Suppose we introduce one hyperplane for each constraining curve from \ref{eq:C}, we end up with polynomially many hyperplanes. 
However, we must compute each intersection, resulting in $n!$ many vertices and naturally exponential growth. 

Ultimately, we also want to point out other possibilities we have experimented with.
\begin{itemize}
    \item Extended formulation. As discussed in the previous section, more complex extended formulations may help solve this 
 problem. 
    \item Hyperplane Arrangements. Recall the results from data processing; we can witness a clear separation of vertices in the vector. 
 This property may be helpful for hyperplane arrangements, but there is no direct existing research from this aspect. 
    \item Deep Learning. Deep learning is always helpful in prediction. It may be helpful in predicting the number of facets or in the
 computation of our mathematical program. However, the interpretation of those results in formal mathematics remains a challenge. 
\end{itemize}

\bibliographystyle{plain}
\bibliography{../documentation/citations}

\end{document}
    