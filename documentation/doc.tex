\documentclass[12pt,letterpaper]{article}

\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{cleveref}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{float}

\newcommand*{\bigo}{\mathcal{O}}
\newcommand*{\R}{\mathbb{R}}
\newcommand{\Poly}{\mathcal{P}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}[theorem]{Observation}

\setlength\parindent{0pt}

\title{Computing the convex hull of solution vectors to the airplane refueling problem to find a general description}
\author{Zixuan Fan}

\begin{document}
\maketitle

\section{Motivation}

\subsection{Problem Statement}
The Airplane Refueling Problem is a combinatorial problem that is 
neither known to be NP-complete nor polynomially solvable. 
It reflects a real-life problem, where a fleet of airplanes refuels each other 
to maximize the flying distance. The earliest mention of this problem stems 
from \textit{Puzzle-Math} \cite{gamow1958puzzle} by George Gamow and Marvin Stern published in 1958.    
Woeginger introduced the a formalization of this problem, 
we present a definition based on his work. \cite{woeginger2010scheduling}
\begin{definition}
 Given $n$ planes each with a volume $w_j$ and consumption rate $p_j$,
 what is the longest time/distance that this fleet of planes can fly with full volumes,
 if they can refuel each other?
\end{definition}
An intuitive idea is consuming the volume of only one plane 
at a time, and dropping it out after its volume is empty. This approach 
finds a \textit{drop out ordering} $\sigma$
that maximizes the objective function 
\begin{align*}
    \max \sum_{j=1}^{n} \dfrac{w_{\sigma(j)}}{\sum_{k = j}^n p_{\sigma(k)}}
\end{align*}
To elaborate on this objective function, only the planes with indexing no less than $j$ in order $\sigma$
are consuming the volume of the $j$-th plane. Thus, the time until the $j$-th fuel is fully consumed is 
\begin{align*}
    \dfrac{w_{\sigma(j)}}{\sum_{k = j}^n p_{\sigma(k)}}
\end{align*}
while the sum of all these times is the objective maximization function.

\subsection{Scheduling and Mathematical Programming}
Based on understaning mentioned above, Wiebke HÃ¶hn introduced a formalisation of this problem as a scheduling problem. \cite{hohn2015performance}
We would briefly introduce the scheduling problem, and explain how the problem fits in this context. 
\begin{definition}
    Given a cost function $f$ and a sequence of $n$ jobs, each with processing time $p_j$ and weight $w_j$, how to find a permutation $\pi$
    such that the objective function $\sum_{j = 1}^{n} w_j f(\pi, j)$ is minimized or maximized? 
\end{definition}
For simplicity, we also introduce the completion time $C_j$, i.e. the time until the $j$-th plane is consumed given permutation $\pi$.
\begin{align*}
    C_j = \sum_{k = 1}^j p_{\pi(k)}
\end{align*}
In our problem, we can view $\sigma$ as permutation $\pi$, the reverse of ordering.
\begin{align*}
    \sum_{j=1}^{n} \dfrac{w_{\sigma(j)}}{\sum_{k = j}^n p_{\sigma(k)}} = \sum_{j = 1}^n \dfrac{w_{\pi(j)}}{\sum_{k = 1}^j p_{\pi(k)}}
\end{align*}
Replacing the sum of processing time with completion time, we obtain a cost function $f(\pi, j) := 1 / C_{\pi(j)}$
and the objective function in a simpler form.
\begin{align*}
    \sum_{j = 1}^n \dfrac{w_{\pi(j)}}{\sum_{k = 1}^j p_{\pi(k)}} = \sum_{j = 1}^n \dfrac{\pi(j)}{C_{\pi(j)}} =  \sum_{j = 1}^n \dfrac{w_{j}}{C_j}
\end{align*}
With all premises explained, we can finally introduce a mathematical programming formulation. Note that $S_n$ stands for the permutation group of order $n$. 
\begin{align*}
    &\max w^T \cdot x \label{eq:linobj} \tag{A} \\ 
    \text{s. t.\ }& C_{\pi(j+1)} - C_{\pi(j)} = p_{\pi(j)} \forall j \\
    & \pi \text{ is a permutation} \\ 
    & x_j = \dfrac{1}{C_j} \ \ \forall j \in [n] 
\end{align*}
Should this be a linear program, we can solve the problem in polynomial time. 
However, the constraints are not linear, for they are based on the permutations.
Hence, our primitive goal is finding an equivalent linear program to this formulation. In addition, 
we are also interested in the potential of convex programming in this problem. 

\section{Theoretical Backgrounds}
Following the formulation, an intuition is computing the convex hull of feasible solutions, 
which are all possible combinations of the completion time's reciprocals. 
\begin{align*}
 s = \left( \dfrac{1}{C_1}, \dfrac{1}{C_2}, ..., \dfrac{1}{C_n}\right)
\end{align*} 
We can observe that each solution vector corresponds to a permutation of the consumption rate. 
More precisely, the longer the completion time, the later the volume is consumed in the permutation.
There are $n!$ many of these vectors, and the convex hull is a polytope in $\R^n$. 
Thus, we want to explore the special properties of this polytope.

\subsection{Description of the Polytope}
We refer to a convex polytope $\Poly$ as a convex set in $R^n$ Euclidean space describing a polytope. 
There are two ways to describe a polytope.
\begin{enumerate}
    \item $V$-Representation: A convex hull of finite set of points. 
    \begin{align*}
        \Poly = \text{conv}\{v_1, \cdots, v_n \}
    \end{align*}
    \item $H$-Representation: A finite sets of inequalities, i.e. the intersection of half-spaces
    \begin{align*}
        \Poly = \{x \in \R^n | Ax \leq b \}
    \end{align*}
\end{enumerate}
It is known that the two representations are equivalent. 
This is a classical result that can found in many textbooks. 
Hence, we omit the proof for it. 
\begin{theorem}
    \label{thm:polytope}
    For each polytope $\Poly$ in $\R^n$, there is an $H$-polytope $\Poly$'s 
    describing exactly the same polytope as $\Poly$ and vice versa.
\end{theorem}
Recall that we are able to generate a convex hull of a given set of vectors with 
the given inputs of the scheduling problem. This is a $V$-representation of the polytope.
Given Theorem \ref{thm:polytope}, we are interested in an $H$-representation of the same polytope.
More specifically, is there a generalized $H$-representation that is computable in polynomial time 
in the number of dimensions? 
To answer this question, we took a closer look at 
a few concrete examples in section 3.


\subsection{Permutation Matrices}
In addition to the two representations of the polytope, we also found a special type of polytope 
in literature review and deemed it helpful for our problem. Before introducing this polytope in details, 
we first introduce the concept of permutation matrices.
\begin{definition}
    A permutation matrix $M \in \R^{n \times n}$ is a square binary matrix describing a permutation $\pi$.
    For each entry $m_{ij}$ of $M$, the following constraints must hold.
    \begin{enumerate}
        \item $m_{ij} \in \{0, 1\}$ for all $i, j$
        \item $\sum_{i = 1}^n m_{ij} = 1$, for all $j$
        \item $\sum_{j = 1}^n m_{ij} = 1$, for all $i$
    \end{enumerate}
    For simplicity, we denote $m_{ij} = 1$ as 1-entry while $m_{ij} = 0$ as 0-entry in the permutation matrix.
\end{definition}
First, we show that there is a bijective relationship between permutation matrices and permutations, i.e. permutation matrices 
can be used to describe permutations.
\begin{lemma}
    There is a bijective mapping between permutation matrices and permutations.
\end{lemma}
\begin{proof}
    For each permutation $\pi \in S_n$, we construct a permutation matrix $M$.
    \begin{align*}
        m_{ij} = 
        \begin{cases}
            1 & \text{if } \pi(i) = j \\
            0 & \text{otherwise}
        \end{cases}
    \end{align*}
    We start with \textbf{injectivity}. Without loss of generality, assume there are two distinct permutations $\pi$, $\pi'$ and conrrespondingly 
    constructed permutation matrices $M$, $M'$. We show that $M \neq M'$. Suppose for contradiction that $M = M'$.
    Then, for each $m_{ij} = 1$ it follows $\pi(i) = j$ and $\pi'(i) = j$. Analogously, for each $m_{ij} = 0$, it holds $\pi(i) \neq j$ and $\pi'(i) \neq j$.
    This implies $\pi(i) = \pi'(i)$ for all $i \in [n]$. It follows $\pi = \pi'$, which is a contradiction. \\\\
    Then, we show $\textbf{surjectivity}$. Again, suppose for contradiction that there exists a permutation matrix $M$ such that 
    it cannot be constructed from any permutation $\pi$. \\
    By the first constraint in definition, each entry in $M$ is either zero or one. Furthermore, 
    the second and the third constraints imply that there is exactly one 1-entry in each row and each column,
    otherwise the row sum or column sum would be strictly greater than one.
    This allows us to reconstruct a permutation $\pi$, where $M$ can be constructed from. 
    \begin{align*}
        \pi(i) = j \text { if } m_{ij} = 1
    \end{align*}
    The construction is well-defined, because $m_{ik} = 0$ for all $k \neq j$ and $m_{ij} = 0$.
    This is a contradiction, and we conclude can conclude the \textbf{bijectivity}.
\end{proof}
A property of permutation matrices is that they are doubly stochastic, i.e., the sum of each row and each column is 1.
In the special case of permutation matrices, the matrix is also orthogonal.
\begin{lemma}
    If $M$ is a permutation matrix, then $M$ is orthogonal.
\end{lemma}
\begin{proof}
    We consider the matrix product $MM^T$. It suffices to show $MM^T = I$. The $(i, j)$-th entry of $MM^T$ is given by 
    \begin{align*}
        (MM^T)_{ij} = \sum_{k = 1}^n m_{ik}m_{jk}
    \end{align*}
    If $i \neq j$, then we are summing up the product of entries in two different rows. 
    Observe that the 1-entries in two different rows are not in the same column.
    Since there is only one 1-entry in each column, the products are all zero. Hence, their sum is also zero. \\ 
    If $i = j$, then we are summing up the product of entries in the same row.
    There is only one 1-entry in each row, so the product is one. \\
    In conclusion, the matrix product is the identity matrix, and $M$ is orthogonal.
\end{proof}
Directly from orthogonality, we can derive the inverse of a permutation matrix.
\begin{corollary}
    \label{cor:inverse}
    If $M$ is a permutation matrix, then $M^{-1} = M^T$.
\end{corollary}


\subsection{Permutahedron}
A permutahedron is a polytope that is the convex hull of all vertices representing a permutation i.e. $\{(\pi(1), \pi(2), \cdots, \pi(k) | \pi \in S_n) \}$. \cite{doi:10.1137/0122054}
In the permutahedron of the $n$-dimension $P_n$, each vertex is a permutation of $n$ elements.
Hence, there are $n!$ many vertices in $P_n$. For example, a hexagon is a three-dimensional permutahedron. 
First, we take a look at a simple LP formulation of permutahedra, provided by Bowman and Joseph. \cite{bowman1972permutation}
\begin{align*}
    \sum_{i \in S} x_i = \sum_{k = n - |S| + 1}^n k, \ \ \forall \emptyset \neq S \subset [n]
\end{align*}
We need $2^n - 2$ facets to define the polytope.\\
Although the number of faces is not polynomial in $n$,
some symmetric properties were discovered and are useful for optimization. Some researchers \cite{bowman1972permutation}
have found an extended formulation $\R^{n} \rightarrow \R^{n^2 + n}$, which allows us 
to solve the LP on $P_n$ in polynomial time. 

\begin{align}
    &\sum_{j \in [n]} y_{ij} = 1, \ \ \forall i \in [n], \sum_{i \in [n]} y_{ij} = 1, \ \ \forall j \in[n] \label{eq:C} \tag{B} \\ 
    &y_{ij} \geq 0, \ \ \forall i, j \in [n] \nonumber 
\end{align}
The trick is the permutation matrix. We show that this is a formulation of the 
convex hull of all permutation metrices.
\begin{lemma}
    The polytope defined by \ref{eq:C} is the convex hull of all permutation matrices to order $n$.
\end{lemma}
\begin{proof}
    It suffices to show that \ref{eq:C} describes the set of all convex combinations of permutation matrices. \\
    $\impliedby$: Let $M^1, M^2, \cdots M^k$ be arbitrary distinct permutation matrices. Let $\lambda_1, \lambda_2, \cdots \lambda_k \in [0, 1]$ be arbitrary
    such that $\sum_{i=1}^k = 1$.
    We obtain a convex combination $Y = \sum_{l = 0}^k \lambda_l M^i$.We check each constraints accordingly. 
    \begin{align*}
    \sum_{j \in [n]} y_{ij} &= \sum_{j \in [n]} \sum_{l = 0}^k \lambda_l m_{ij}^l \\
    &= \sum_{l = 0}^k \lambda_l \sum_{j \in [n]} m_{ij} = \sum_{l = 0}^k \lambda_l = 1 \\ 
    \sum_{i \in [n]} y_{ij} &= \sum_{i \in [n]} \sum_{l = 0}^k \lambda_l m_{ij}^l \\
    &= \sum_{l = 0}^k \lambda_l \sum_{i \in [n]} m_{ij} = \sum_{l = 0}^k \lambda_l = 1 \\ 
    y_{ij} &= \sum_{l = 0}^k \lambda_k m_{ij}^l \geq \sum_{l = 0}^k \lambda_k \cdot 0  = 0 
    \end{align*}
    We may conclude that $Y$ is in the polytope described by \ref{eq:C}. \\
    $\implies$: Let $Y$ be a feasible solution to constraints in \ref{eq:C}. 
    We observe that $Y$ is doubly stochastic. It is shown by Birkhoff's Theorem \cite{jurkat1967term} that 
    $Y$ is a convex combination of permutation matrices. 
\end{proof}

As a consequence, we can obtain the vertices representing the permutations by 
\begin{align*}
    x_i = \sum_{j = 0}^n j \cdot y_{ij} \forall i \in[n]
\end{align*}
In this way, we are able to reduce the number of constraints to $n^2 + 3n$ under $\R^{n^2 + n}$.
We will show how this approach of extended formulation helped us in Section 4. 

\section{Attempts with linear programming}
As a start of this project, 
we explored the possibility of find the $H$-representation of the given polytope $\Poly$
given the $V$-representation of it. 
As the description of the scheduling problem does not contain any vertices, our first step is 
generating the $V$-represention. 

\paragraph{Setups}
For the computation of the polytope with vertices, we use the scientific computing software polymake \cite{assarf2017computing}.
Polymake is a software that can compute the $H$-representation of a polytope given the $V$-representation. 
All other data processing, data analytics and machine learning tasks were done with Python.
The computation was run on a private computer with Intel i5-12600k CPU and 16 GB RAM. 


\subsection{Input Generation}
Since the completion time is based on the permutation $\pi$, we need a few steps to generate all possible 
vectors. 
\begin{enumerate}
    \item Generate a randomized/special processing time $p_j$
    \item Pick a permutation $\pi$, arrange the consumption time in the order of $\pi$
    \item Compute the completion time by $C_1 = p_1$ and $C_{i+1} = C_i + p_{i+1}$
    \item Compute the inverse of $C_j$
    \item Repeat until all permutations are visited
\end{enumerate}
This results in $n!$ vertices in $\R^n$. A straightforward traversal on the vertices is not 
polynomial-time, but it may help us to find potentially polynomial-time $H$-representation within the polytope $\Poly$.

\subsection{First Computation}
\paragraph{Direct Observations}
Having tested a few randomly generated inputs, we have made some direct observations.
When several identical processing times are generated, the number of facets is largely reduced.
Suppose $n$ processing times were generated, while there were only $m$ unique values. 
The resulting polytope can be reduced to a polytope in $\R^m$.\\
The reason is also straightforward. If two identical values were generated, 
the number of unique solution vectors will also be reduced. Let $i$ and $j$ be 
arbitrary indices such that $p_i = p_j$. For each value of $C_i$ in permutation $\pi$, 
there is a $C'_j$ of the same value in another permutation $\pi'$. 
$C'_j$ and $\pi'$ can be generated by simply swapping $i$ and $j$ in the permutation $\pi$. \\
In conclusion, it makes sense that our computational experiments should be based 
on the number of unique values in the processing time. 
Thus, we only generated unique processing times in the following experiments. 

\paragraph{Running Time in Practice}
Although the computation time of the $H$-representation is polynomial in the number of vertices, our computation takes 
a significant amount of time. The reason is that the number of vertices is exponential in $n$. 
For the first few dimensions, the computation was quick and the results were as expected.
When we come to the fifth dimension, the computation takes a few seconds, while the sixth dimension 
takes around 1 minute to terminate. The computation of a 7-dimensional polytope does not terminate within 24 hours. 
This largely stops us from further computation in even higher dimensions. 
Luckily, we have obtained some raw data of the first 6 dimensions. 
The number of facets do not resemble a polynomial growth, hence we would expect an exponential growth in this case.

 
\subsection{Mass Computation for patterns}
The exponential growth of facets does not mean an exponential decision prodecure for the polytope. 
Our second trial serves as mass computation in lower dimensions, where we want to obtain 
as many data as possible. In total, we have generated 10,000 inputs for 4-dimensional cases, 
and 1,000 inputs for 5-dimensional cases. \\
Based on those data, we would like to find a pattern in 
the number of facets. Thus, we establish a large amount of 
computation in $n = 4$ and $n = 5$. Some direct oberservations are shown in the following table. 
\begin{table}[ht]
    \centering
    \begin{tabular}{||c | c | c ||}
        \hline 
        $n$ & Number of Facets & Number of Unique Values\\
        \hline 
        \hline 
 2 & 2 & 1\\
        \hline 
 3 & 8 & 1\\
        \hline 
 4 & 67, 68, 69, 70 & 4\\
        \hline 
 5 & $700 \sim 800$ & $\geq 66$\\
        \hline 
 6 & $10000 \sim 12000$  & Unknown\\
        \hline 
    \end{tabular}
    \caption{Number of facets in the polytope for $n \leq 6$}
\end{table}
Starting from $n = 4$, the number of facets is no longer unique. 
Hence, we guess that the number of facets is somehow related to the input chosen. 
Unfortunately, a direct pattern was not observed for $n=5$.
One interesting case for $n = 4$ is when the number of facets is 67.
Among 10,000 inputs, only one input results in 67 facets. 

\paragraph{A Special Input}
We would like elaborate on this case to explain what we have observed.
The generated input for this 4d case is
\begin{align*}
    p_0 = (26, 35, 53, 79)
\end{align*}
By brute-force, we are able to obtain all possible values for each in dimension in 
our polytope. One numerical observation is $26+53 = 79$. If we recall 
the direction observation in the previous part, the same values on different dimensions 
reduces the complexity of the polytope. In our results, the number of unique vectors is not reduced.
However, such relations reduces the number of higher dimensional faces. 
As a result, we have fewer facets.\\
Then we also tried to reproduce this result, it was discovered that the vector $p$ 
in satisfying the following property will result in the exactly 67 facets.
\begin{align*}
    p = (a, b, 3b - 2a, 3b - a)
\end{align*}
One simplest example is $p = (1, 2, 4, 5)$. 
We still fail to derive more general results from this observation.


\subsection{Machine Learning, Visualization and Discoveries}
Since no direct pattern was observed, we would like to use machine learning techniques to find a pattern in the data.
Both supervised and unsupervised learning techniques were exploited to find the pattern in the data. 
We focus on the case of $n = 4$, where the unique number of facets is few, and the data are easy to visualize.
We randomly generate each input of $v \in \R^4$ and call the Polymake interface for the polytope. In the end, 
we also process that data such that each dimension is a ratio between its original value and the first dimension. 
\begin{align*}
 (v_1, v_2, v_3, v_4) \rightarrow \left(1, \frac{v_2}{v_1}, \frac{v_3}{v_1}, \frac{v_4}{v_1} \right)
\end{align*}
We applied regressions, discriminant analysis, tree-based models on the data. 
None of them proved to generate promising results. \\\\

Aside from machine learning, we also visualized the preprocessed data to see 
if any pattern was observed, as shown in the figure below. 
A clear separation can be witnessed, but we cannot find a smooth curve separating the data, for outliers reside 
in the middle of the other category. 
\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.6]{3d_plot.png}
\end{figure*}
On the other hand, we also observed an interesting property through visualization. If we consider a solution vector 
\begin{align*}
 s = \left( \dfrac{1}{C_1}, \dfrac{1}{C_2}, ..., \dfrac{1}{C_n} \right)
\end{align*}
For every two dimensions $ i \neq j$, the volume of the $i-$th plane is consumed before the $j-$th plane, or vice versa.
Formally speaking, either $C_1 < C_2$ or vice versa, and there is an equal amount of them, separated by the hyperplane 
$x_i - x_j = 0$. This separation is a valuable property and an interesting technique called \textbf{hyperplane arrangements} 
endeavors to research it. Unfortunately, we have not found a direct application of this technique that is useful for our problem.


\section{Attempts with non-linear programming}
Since we failed to find a direct pattern of $H$-representation with linear programming, 
we returned to the original formalisation of them problem, and would like to 
find an equivalent convex formulation of the problem.
Inspired by the permutahedron, we wanted to find a similar formulation for the polytope of the completion time,
because there is a permutation-related structure in the polytope. 

\subsection{Formulation}
Recall in \ref{eq:linobj}, the completion time $C_j$ is 
an acculumating sum of the processing time $p_j$ with regard to 
permutation $\pi$. We would exploit the formulation in \ref{eq:C} 
to find a similar formulation for the completion time. The figure \ref{fig:completion}
shows how the completion time is constructed with a permutation, and respectively with a permutation matrix.
\begin{figure}[h!]
    \centering
    \begin{tikzcd}
        \begin{pmatrix}
            p_1 \\ 
            \vdots \\
            p_n
        \end{pmatrix} 
        \arrow[r, "\pi"]
        & \begin{pmatrix}
            p_{\pi(1)} \\ 
            \vdots \\
            p_{\pi(n)}
        \end{pmatrix}
        \arrow[r, "sum"]
        & \begin{pmatrix}
            p_{\pi(1)} \\ 
            \vdots \\
            \sum_{i = 0}^n p_{\pi(i)}
        \end{pmatrix} = 
        \begin{pmatrix}
            C_{\pi(1)} \\
            \vdots  \\
            C_{\pi(n)}
        \end{pmatrix}
        \arrow[r, "\pi^{-1}"]
        & \begin{pmatrix}
            C_1 \\ 
            \vdots \\
            C_n
        \end{pmatrix}
    \end{tikzcd}
    \caption{Construction of completion time}
    \label{fig:completion}
\end{figure}
Apparently, such operations can be formulated with matrices. 
We introduce a matrix $S$ for the summation operation. $S$ is a lower-triangular matrix with all ones.
In addition, we retain the permutation matrix as shown in \ref{eq:C}.
For naming conventions of variables, we use $y_i$ for completion and $z_i$ for permutation matrices 
in the following formulation.
\begin{align*}
    \max w^Tx& \text{ s.t. } \label{eq:D} \tag{C} \\
 x_i y_i &= 1 \ \forall i, \ \ y \geq 0, \\ 
    \sum_{i \in [n]} z_{ij} &= 1 \forall j,  \ \ 
    \sum_{j \in [n]} z_{ij} = 1 \forall i,  \ z_{ij} \in \{0, 1\} \forall i, j \\ 
    \Pi &= (z_{ij}), S = \begin{pmatrix}
        1 & \cdots & 0 \\ 
        \cdots & \cdots & \cdots \\ 
        1 & \cdots & 1
    \end{pmatrix} \\
 y &= \Pi^T S \Pi p 
\end{align*}
One special part of the formulation is $\Pi^T$, where the transpose 
of the permutation matrix is used. This stems from the result in \ref{cor:inverse},
where the inverse of a permutation matrix is its transpose.

\subsection{Convexity}
Now that we have a correct formulation, we are ready to implement and execute some experiments.
However, the given formulation serves as a mixed-integer nonlinear program. 
For actual execution, we have first convert it into a non-integer program, and check its convexity afterwards.
In a classical manner, the binary integer variables are relaxed to continuous variables.
\begin{align*}
 z_{ij} \in [0, 1] \ \forall i, j
\end{align*}
There are also quadratic equality constraints, where we lose the convexity for the equal relation. 
There are roundabouts to make them convex, by introducing inequalities. 
\begin{align*}
 x_i y_i &\geq 1 \ \forall i \\
 y &\geq \Pi^T S \Pi p
\end{align*}
While we can easily agree that linear constraints are convex, we would like to 
show that these quandratic inequalities are convex.
\begin{lemma}
    The introduced quadratic inequalities are convex.
\end{lemma}
\begin{proof}
    We start with the first inequality. Let $i, j$ be arbitrary.
    By assumption, it holds that $x_iy_i \geq 1$, $x_jy_j \geq 1$ and $x, y \geq 0$.
    For any $\lambda \in [0, 1]$, it follows 
    \begin{align*}
        &(\lambda x_i + (1-\lambda) x_j)(\lambda y_i + (1-\lambda) y_j) \\
        &= \lambda^2 x_i y_i + (1-\lambda)^2 x_j y_j + \lambda(1-\lambda)(x_iy_j + x_jy_i) \\
        &\geq \lambda^2 + (1-\lambda)^2 + \lambda(1-\lambda)(x_iy_j + x_jy_i) \\ 
        &\geq \lambda^2 + (1-\lambda)^2 + \lambda(1-\lambda)(\dfrac{y_j}{y_i} + \dfrac{y_i}{y_j}) \\ 
        &\geq \lambda^2 + (1-\lambda)^2 + 2\lambda(1-\lambda) \\ 
        &= \lambda^2 + 1 - 2\lambda(1-\lambda) + \lambda^2 + 2\lambda(1-\lambda) \\ 
        &= 1 + 2\lambda^2 \geq 1
    \end{align*}
    Now we consider the second inequality. Recall that $\Pi^T = \Pi^{-1}$.
    We can rewrite the inequality as
    \begin{align*}
        \Pi y - S\Pi p \geq 0
    \end{align*}
    Note that the term $S\Pi p$ is linear for $z$'s and $y$'s.
    Thus, we only have deal with the term $\Pi y$. We take a closer look at this product.
    \begin{align*}
        \Pi y &= \begin{pmatrix}
            \sum_{j = 0}^{n} z_{1j}y_j \\
            \vdots \\
            \sum_{j = 0}^{n} z_{nj}y_j
        \end{pmatrix}
    \end{align*}
    Analogously to the first inequality, we can show that $z_{ij}y_j \geq 0$ is convex. 
    By adding up all these inequalities, we obtain that $\sum_{j = 0}^{n} z_{ij}y_j \geq 0$ is also convex.
    In conclusion, the inequality $\Pi y - S\Pi p \geq 0$ is convex.
\end{proof}
In this way, we can obtain a feasible, convex, but \textbf{unbounded} area. 
\begin{lemma}
    The feasible are of the given convex program is unbounded.
\end{lemma}
\begin{proof}
    Suppose for contradiction that the feasible area is bounded.
    Specifically, we would show that $y$ is unbounded. Hence, 
    there exists by assumption a constant $M$ such that $|y| \leq M$ for all 
    feasible $y$. \\ 
    Let $\hat{y}$ be arbitrarily feasible. We construct a new feasible $\hat{y}'$ by 
    \begin{align*}
        \hat{y}' = \hat{y} + M \cdot \vec{\mathbf{1}}
    \end{align*} 
    Now we check all three constraints concerning $\hat{y}'$.
    We start with non-negativity.
    \begin{align*}
        \hat{y}' &= \hat{y} + M \cdot \vec{\mathbf{1}} \geq \hat{y} \geq 0
    \end{align*}
    For $x_iy_i \geq 1$, increasing $\hat{y}_i$ by $M$ also results in a valid $\hat{y}_i$.
    Analogously, the second inequality is also satisfied. Thus, $\hat{y}'$ is feasible. \\
    It follows 
    \begin{align*}
        |\hat{y}'| = |\hat{y} + M \cdot \vec{\mathbf{1}}| \geq |\hat{y}| + M \cdot n > M
    \end{align*}
    which is a contradiction. Hence, the feasible area is unbounded.
\end{proof}

\textbf{Unboundedness} is another problem, because we objective function is maximized. 
Thus, any attempts on convex optimization would maximize the objective function to infinity.
This discovery also hints that relaxing this non-convex problem 
to a convex version might result in an unbounded feasible area.
Unfortunately, we cannot solve the problem with this non-linear programming approach. 

\subsection{Simple Practical Experiments}
Although the program is not convex, we can still run it in some scientific computing software.
For software based on Disciplined Convex Programming, like CVXPY \cite{diamond2016cvxpy}, we cannot even run the program 
because it has semantic checks on convexity. For software that does not require this check, like SCIP \cite{BolusaniEtal2024ZR}, the program 
ends up in a dead loop without termination. 

\section{Summary}
In this project, we reviewed a few pieces of literature on the airplane refueling problem and 
attempted to find a mathematical programming formulation for the problem. 
We gave a valid formulation, but it was not convex, and we were not able to solve it in polynomial time. 
This intractability is also evidence showing that this direction may not work.
\begin{enumerate}
    \item On the one hand, if we try to find a linear programming formulation, in other words, an $H$-representation, 
 it takes us exponential time to compute all facets. 
    \item On the other hand, if we try to formulate this problem as a non-linear program problem, we end up 
 with a maximization problem over an unbounded convex feasible area or a bounded non-convex area. 
\end{enumerate}
We can observe the second point from permutahedra, and we want to elaborate on the first point. 
Suppose we introduce one hyperplane for each constraining curve from \ref{eq:C}, we end up with polynomially many hyperplanes. 
However, we must compute each intersection of curves. Since vertices are primarily also intersections, 
we would expect an exponential number of intersections, given that we already have $n!$ many vertices. 
Ultimately, we also want to point out other possibilities that may be worth trying in future works.
\begin{itemize}
    \item Extended formulation. As discussed in the previous section, more complex extended formulations may help solve this 
 problem. 
    \item Hyperplane Arrangements. Recall the results from data processing; we can witness a clear separation of vertices in the vector. 
 This property may be helpful for hyperplane arrangements, but there is no direct existing research from this aspect. 
    \item Deep Learning. In some cases of non-convex probelms, deep learning is helpful in finding the exact solution. 
    It may be helpful in predicting the number of facets or in the  computation of our mathematical program. 
    However, the interpretation of the deep learning results in formal mathematics remains a challenge. 
\end{itemize}

\bibliographystyle{plain}
\bibliography{../documentation/citations}

\end{document}
    